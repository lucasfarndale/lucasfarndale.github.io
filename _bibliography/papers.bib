---
---

@InProceedings{farndale2023more,
author={Farndale, Lucas
and Insall, Robert
and Yuan, Ke},
editor={Cao, Xiaohuan
and Xu, Xuanang
and Rekik, Islem
and Cui, Zhiming
and Ouyang, Xi},
title={More from Less: Self-supervised Knowledge Distillation for Routine Histopathology Data},
booktitle={Machine Learning in Medical Imaging},
year={2023},
publisher={Springer Nature Switzerland},
address={Cham},
pages={454--463},
abstract={Medical imaging technologies are generating increasingly large amounts of high-quality, information-dense data. Despite the progress, practical use of advanced imaging technologies for research and diagnosis remains limited by cost and availability, so more information-sparse data such as H{\&}E stains are relied on in practice. The study of diseased tissue would greatly benefit from methods which can leverage these information-dense data to extract more value from routine, information-sparse data. Using self-supervised learning (SSL), we demonstrate that it is possible to distil knowledge during training from information-dense data into models which only require information-sparse data for inference. This improves downstream classification accuracy on information-sparse data, making it comparable with the fully-supervised baseline. We find substantial effects on the learned representations, and pairing with relevant data can be used to extract desirable features without the arduous process of manual labelling. This approach enables the design of models which require only routine images, but contain insights from state-of-the-art data, allowing better use of the available resources.},
isbn={978-3-031-45673-2},
url={https://link.springer.com/chapter/10.1007/978-3-031-45673-2_45},
arxiv={2303.10656},
selected={true}
}

@article{farndale2023trident,
  title={TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology},
  author={Farndale, Lucas and Insall, Robert and Yuan, Ke},
  journal={arXiv preprint arXiv:2312.02111},
  year={2023},
  arxiv={2312.02111},
  url={https://arxiv.org/abs/2312.02111},
  abstract={Computational pathology models rarely utilise data that will not be available for inference. This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics. We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance. We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations. In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%. Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines. TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs.},
  selected={true}
}

@article{farndale2024synthetic,
  title={Synthetic Privileged Information Enhances Medical Image Representation Learning},
  author={Farndale, Lucas and Walsh, Chris and Insall, Robert and Yuan, Ke},
  journal={arXiv preprint arXiv:2403.05220},
  year={2024},
  arxiv={2403.05220},
  url={https://arxiv.org/abs/2403.05220},
  abstract={Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction).},
  selected={true}
}

@article{farndale2025divide,
  title={Divide and Conquer Self-Supervised Learning for High-Content Imaging},
  author={Farndale, Lucas and Henderson, Paul and Roberts, Edwards W and Yuan, Ke},
  journal={arXiv preprint arXiv:2503.07444},
  year={2025},
  arxiv={2503.07444},
  url={https://arxiv.org/abs/2503.07444},
  abstract={Self-supervised representation learning methods often fail to learn subtle or complex features, which can be dominated by simpler patterns which are much easier to learn. This limitation is particularly problematic in applications to science and engineering, as complex features can be critical for discovery and analysis. To address this, we introduce Split Component Embedding Registration (SpliCER), a novel architecture which splits the image into sections and distils information from each section to guide the model to learn more subtle and complex features without compromising on simpler features. SpliCER is compatible with any self-supervised loss function and can be integrated into existing methods without modification. The primary contributions of this work are as follows: i) we demonstrate that existing self-supervised methods can learn shortcut solutions when simple and complex features are both present; ii) we introduce a novel self-supervised training method, SpliCER, to overcome the limitations of existing methods, and achieve significant downstream performance improvements; iii) we demonstrate the effectiveness of SpliCER in cutting-edge medical and geospatial imaging settings. SpliCER offers a powerful new tool for representation learning, enabling models to uncover complex features which could be overlooked by other methods.},
  selected={true}
}
