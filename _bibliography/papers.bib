---
---

@InProceedings{farndale2023more,
author={Farndale, Lucas
and Insall, Robert
and Yuan, Ke},
editor={Cao, Xiaohuan
and Xu, Xuanang
and Rekik, Islem
and Cui, Zhiming
and Ouyang, Xi},
title={More from Less: Self-supervised Knowledge Distillation for Routine Histopathology Data},
booktitle={Machine Learning in Medical Imaging},
year={2023},
publisher={Springer Nature Switzerland},
address={Cham},
pages={454--463},
abstract={Medical imaging technologies are generating increasingly large amounts of high-quality, information-dense data. Despite the progress, practical use of advanced imaging technologies for research and diagnosis remains limited by cost and availability, so more information-sparse data such as H {\&}E stains are relied on in practice. The study of diseased tissue would greatly benefit from methods which can leverage these information-dense data to extract more value from routine, information-sparse data. Using self-supervised learning (SSL), we demonstrate that it is possible to distil knowledge during training from information-dense data into models which only require information-sparse data for inference. This improves downstream classification accuracy on information-sparse data, making it comparable with the fully-supervised baseline. We find substantial effects on the learned representations, and pairing with relevant data can be used to extract desirable features without the arduous process of manual labelling. This approach enables the design of models which require only routine images, but contain insights from state-of-the-art data, allowing better use of the available resources.},
isbn={978-3-031-45673-2},
url={https://arxiv.org/abs/2303.10656},
selected={true}
}


