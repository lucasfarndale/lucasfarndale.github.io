---
---

@InProceedings{farndale2023more,
author={Farndale, Lucas
and Insall, Robert
and Yuan, Ke},
editor={Cao, Xiaohuan
and Xu, Xuanang
and Rekik, Islem
and Cui, Zhiming
and Ouyang, Xi},
title={More from Less: Self-supervised Knowledge Distillation for Routine Histopathology Data},
booktitle={Machine Learning in Medical Imaging},
year={2023},
publisher={Springer Nature Switzerland},
address={Cham},
pages={454--463},
abstract={Medical imaging technologies are generating increasingly large amounts of high-quality, information-dense data. Despite the progress, practical use of advanced imaging technologies for research and diagnosis remains limited by cost and availability, so more information-sparse data such as H{\&}E stains are relied on in practice. The study of diseased tissue would greatly benefit from methods which can leverage these information-dense data to extract more value from routine, information-sparse data. Using self-supervised learning (SSL), we demonstrate that it is possible to distil knowledge during training from information-dense data into models which only require information-sparse data for inference. This improves downstream classification accuracy on information-sparse data, making it comparable with the fully-supervised baseline. We find substantial effects on the learned representations, and pairing with relevant data can be used to extract desirable features without the arduous process of manual labelling. This approach enables the design of models which require only routine images, but contain insights from state-of-the-art data, allowing better use of the available resources.},
isbn={978-3-031-45673-2},
url={https://link.springer.com/chapter/10.1007/978-3-031-45673-2_45},
arxiv={2303.10656},
selected={true}
}

@article{farndale2023trident,
  title={TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology},
  author={Farndale, Lucas and Insall, Robert and Yuan, Ke},
  journal={arXiv preprint arXiv:2312.02111},
  year={2023},
  arxiv={2312.02111},
  url={https://arxiv.org/abs/2312.02111},
  abstract={Computational pathology models rarely utilise data that will not be available for inference. This means most models cannot learn from highly informative data such as additional immunohistochemical (IHC) stains and spatial transcriptomics. We present TriDeNT, a novel self-supervised method for utilising privileged data that is not available during inference to improve performance. We demonstrate the efficacy of this method for a range of different paired data including immunohistochemistry, spatial transcriptomics and expert nuclei annotations. In all settings, TriDeNT outperforms other state-of-the-art methods in downstream tasks, with observed improvements of up to 101%. Furthermore, we provide qualitative and quantitative measurements of the features learned by these models and how they differ from baselines. TriDeNT offers a novel method to distil knowledge from scarce or costly data during training, to create significantly better models for routine inputs.},
  selected={true}
}
